{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "import logging # TBK: logging module\n",
    "import logging.config # TBK\n",
    "import configparser # TBK: To read config file\n",
    "import tqdm # TBK\n",
    "from time import time\n",
    "import psutil\n",
    "from multiprocessing import Pool\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(model_file, input_dir):\n",
    "    model = tf.keras.models.load_model(model_file)\n",
    "\n",
    "    pad(input_dir)\n",
    "    images = []\n",
    "    image_files = []\n",
    "    for img in os.listdir(input_dir):\n",
    "        image_files.append(img)\n",
    "        img = tf.keras.preprocessing.image.load_img(input_dir+img, target_size=(128, 128), color_mode='grayscale')\n",
    "        #img = img.img_to_array(img)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        images.append(img)\n",
    "    images = np.vstack(images)\n",
    "\n",
    "\n",
    "    predictions = model.predict(images)\n",
    "    prediction_labels = np.argmax(predictions, axis=-1)\n",
    "    np.savetxt('prediction.csv', predictions, delimiter=',')\n",
    "\n",
    "    with open('prediction_names.csv', newline='', mode='w') as csvfile:\n",
    "        csvwritter = csv.writer(csvfile, delimiter='\\n')\n",
    "        csvwritter.writerow(image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(num_classes, img_height, img_width):\n",
    "\n",
    "    #strategy = tf.distribute.MirroredStrategy()\n",
    "    #with strategy.scope():\n",
    "    model = ResNet18([img_height, img_width, 1], config['training']['model_name'], num_classes)\n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18(input_shape, name, num_classes):\n",
    "    BN_AXIS = 3\n",
    "\n",
    "    img_input = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Rescaling(-1. / 255, 1)(img_input)\n",
    "    x = tf.keras.layers.RandomRotation(2)(x)\n",
    "    x = tf.keras.layers.RandomFlip(\"horizontal_and_vertical\")(x)\n",
    "\n",
    "    x = tf.keras.layers.ZeroPadding2D(padding=(3, 3), name='conv1_pad')(x)\n",
    "    x = tf.keras.layers.Conv2D(64, (7, 7),\n",
    "                      strides=(2, 2),\n",
    "                      padding='valid',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name='conv1')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=BN_AXIS, name='bn_conv1')(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.ZeroPadding2D(padding=(1, 1), name='pool1_pad')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    x = make_basic_block_layer(x, filter_num=64, blocks=2)\n",
    "    x = make_basic_block_layer(x, filter_num=128, blocks=2, stride=2)\n",
    "    x = make_basic_block_layer(x, filter_num=256, blocks=2, stride=2)\n",
    "    x = make_basic_block_layer(x, filter_num=512, blocks=2, stride=2)\n",
    "\n",
    "\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(img_input, x, name=name)\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_basic_block_base(inputs, filter_num, stride=1):\n",
    "    BN_AXIS = 3\n",
    "    x = tf.keras.layers.Conv2D(filters=filter_num,\n",
    "                                        kernel_size=(3, 3),\n",
    "                                        strides=stride,\n",
    "                                        kernel_initializer='he_normal',\n",
    "                                        padding=\"same\")(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=BN_AXIS)(x)\n",
    "    x = tf.keras.layers.Conv2D(filters=filter_num,\n",
    "                                        kernel_size=(3, 3),\n",
    "                                        strides=1,\n",
    "                                        kernel_initializer='he_normal',\n",
    "                                        padding=\"same\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=BN_AXIS)(x)\n",
    "    x = tf.keras.layers.Dropout(0.25)(x)\n",
    "\n",
    "    shortcut = inputs\n",
    "    if stride != 1:\n",
    "        shortcut = tf.keras.layers.Conv2D(filters=filter_num,\n",
    "                                            kernel_size=(1, 1),\n",
    "                                            strides=stride,\n",
    "                                            kernel_initializer='he_normal')(inputs)\n",
    "        shortcut = tf.keras.layers.BatchNormalization(axis=BN_AXIS)(shortcut)\n",
    "\n",
    "    x = tf.keras.layers.add([x, shortcut])\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def make_basic_block_layer(inputs, filter_num, blocks, stride=1):\n",
    "    x = make_basic_block_base(inputs, filter_num, stride=stride)\n",
    "\n",
    "    for _ in range(1, blocks):\n",
    "        x = make_basic_block_base(x, filter_num, stride=1)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(config):\n",
    "    if int(config['training']['start']) > 0:\n",
    "        return(tf.keras.models.load_model(config['training']['training_dir'], config))\n",
    "    \n",
    "    return(init_model(109, int(config['training']['image_size']), int(config['training']['image_size'])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, config, train_ds, val_ds):\n",
    "    history = model.fit(train_ds,\n",
    "                        validation_data=val_ds,\n",
    "                        epochs=int(config['training']['stop'])-int(config['training']['start']),\n",
    "                        initial_epoch=int(config['training']['start']),\n",
    "                        batch_size = int(config['training']['batchsize']))\n",
    "    \n",
    "    \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_ts(config):\n",
    "    train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        config['training']['scnn_dir'] + '/data',\n",
    "        interpolation='area',\n",
    "        validation_split = 0.2,\n",
    "        subset = \"both\",\n",
    "        seed = 123,\n",
    "        image_size = (int(config['training']['image_size']), int(config['training']['image_size'])),\n",
    "        batch_size = int(config['training']['batchsize']),\n",
    "        color_mode = 'grayscale')\n",
    "    return(train_ds, val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'general' : {\n",
    "        'dir_permissions' : 511\n",
    "    },\n",
    "    'segmentation' : {\n",
    "        'basename' : 'REG',\n",
    "        'segment_processes' : 1,\n",
    "        'overlap' : 0.1,\n",
    "        'max_area' : 400000,\n",
    "        'min_area' : 200,\n",
    "        'delta' : 4,\n",
    "        'flatfield_q' : 0.02\n",
    "    },\n",
    "    'classification' : {\n",
    "        'model_name' : 'Alpha',\n",
    "        'model_dir' : '../../model',\n",
    "        'scnn_instances' : 1,\n",
    "        'fast_scratch' : '/tmp',\n",
    "        'batchsize' : 64,\n",
    "        'image_size' : 128\n",
    "    },\n",
    "    'training' : {\n",
    "        'scnn_dir' : '../../training/20231002',\n",
    "        'model_name': 'Gamma',\n",
    "        'model_path': '../../model/',\n",
    "        'image_size': '128',\n",
    "        'start' : 10,\n",
    "        'stop' : 100,\n",
    "        'validationSetRatio' : 0.2,\n",
    "        'batchsize' : 16,\n",
    "        'seed': 123\n",
    "    }\n",
    "}\n",
    "\n",
    "v_string = \"V2023.10.09\"\n",
    "print(f\"Starting CNN Model Training Script {v_string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training and validation data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_ds, val_ds = init_ts(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize or Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = train_model(model, config, train_ds, val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model and generate confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(config['training']['model_path'] + '/' + config['training']['model_name'])\n",
    "    \n",
    "predictions = model.predict(val_ds)\n",
    "predictions = np.argmax(predictions, axis = -1)\n",
    "y = np.concatenate([y for x, y in val_ds], axis=0)\n",
    "\n",
    "confusion_matrix = tf.math.confusion_matrix(y, predictions)\n",
    "confusion_matrix = pd.DataFrame(confusion_matrix, index = train_ds.class_names, columns = train_ds.class_names)\n",
    "confusion_matrix.to_csv(config['training']['model_path'] + '/' + config['training']['model_name'] + ' confusion.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    \"file\": val_ds.file_paths,\n",
    "    \"label\": val_ds.class_names,\n",
    "    \"prediction\": pd.Series(predictions),\n",
    "}\n",
    "    \n",
    "summary = pd.DataFrame(summary)\n",
    "summary.to_csv(config['training']['model_path'] + '/' + config['training']['model_name'] + ' summary.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
