{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import configparser\n",
    "import logging.config\n",
    "import tqdm\n",
    "import subprocess\n",
    "import datetime\n",
    "from time import time\n",
    "from multiprocessing import Pool, Queue\n",
    "import psutil\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import csv\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Plankline Classification Script V2023.11.13\n"
     ]
    }
   ],
   "source": [
    "directory = '../../analysis/camera1/segmentation/GAK_202207-REG/'\n",
    "\n",
    "config = {\n",
    "    'general' : {\n",
    "        'dir_permissions' : 511\n",
    "    },\n",
    "    'segmentation' : {\n",
    "        'basename' : 'REG',\n",
    "        'segment_processes' : 1,\n",
    "        'overlap' : 0.1,\n",
    "        'max_area' : 400000,\n",
    "        'min_area' : 200,\n",
    "        'delta' : 4,\n",
    "        'flatfield_q' : 0.02\n",
    "    },\n",
    "    'classification' : {\n",
    "        'model_name' : 'Gamma',\n",
    "        'model_dir' : '../../model',\n",
    "        'scnn_instances' : 1,\n",
    "        'fast_scratch' : '/tmp',\n",
    "        'batchsize' : 64,\n",
    "        'image_size' : 128\n",
    "    },\n",
    "    'training' : {\n",
    "        'scnn_dir' : '../../training/20231002',\n",
    "        'model_name': 'Gamma',\n",
    "        'model_path': '../../model/',\n",
    "        'image_size': '128',\n",
    "        'start' : 10,\n",
    "        'stop' : 100,\n",
    "        'validationSetRatio' : 0.2,\n",
    "        'batchsize' : 16,\n",
    "        'seed': 123\n",
    "    }\n",
    "}\n",
    "\n",
    "v_string = \"V2023.11.13\"\n",
    "session_id = str(datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")).replace(':', '')\n",
    "print(f\"Starting Plankline Classification Script {v_string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model_path = f\"../../model/{config['classification']['model_name']}/\"\n",
    "model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Folders and run classification on each segment output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1986 subfolders.\n"
     ]
    }
   ],
   "source": [
    "segmentation_dir = os.path.abspath(directory)  # /media/plankline/Data/analysis/segmentation/Camera1/Transect1-reg\n",
    "classification_dir = segmentation_dir.replace('segmentation', 'classification')  # /media/plankline/Data/analysis/segmentation/Camera1/Transect1-reg\n",
    "classification_dir = classification_dir + '-' + config[\"classification\"][\"model_name\"] # /media/plankline/Data/analysis/segmentation/Camera1/Transect1-reg-Plankton\n",
    "fast_scratch = config['classification']['fast_scratch'] + \"/classify-\" + session_id\n",
    "    \n",
    "os.makedirs(classification_dir, int(config['general']['dir_permissions']), exist_ok = True)\n",
    "os.makedirs(fast_scratch, int(config['general']['dir_permissions']), exist_ok = True)\n",
    "    \n",
    "root = os.listdir(segmentation_dir)\n",
    "\n",
    "print(f\"Found {len(root)} subfolders.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1986/1986 [48:53<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "for r in tqdm.tqdm(root):\n",
    "    images = []\n",
    "    image_files = []\n",
    "    for img in os.listdir(segmentation_dir + '/' + r):\n",
    "        if os.path.splitext(img)[1] == '.png':\n",
    "            image_files.append(img)\n",
    "            img = tf.keras.preprocessing.image.load_img(segmentation_dir + '/' + r + '/' + img,\n",
    "                                                        target_size=(int(config['classification']['image_size']),int(config['classification']['image_size'])),\n",
    "                                                        color_mode='grayscale')\n",
    "            img = np.expand_dims(img, axis=0)\n",
    "            images.append(img)\n",
    "    images = np.vstack(images)\n",
    "    \n",
    "    predictions = model.predict(images, verbose = 0)\n",
    "    prediction_labels = np.argmax(predictions, axis=-1)\n",
    "    df = pd.DataFrame(predictions, index=image_files)\n",
    "    df.to_csv(classification_dir + '/' + r + '_' + 'prediction.csv', index=True, header=True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
